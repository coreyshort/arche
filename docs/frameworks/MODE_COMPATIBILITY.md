# Mode Compatibility & Integration Guide

**Purpose:** Document how arche modes work together, when they complement each other, and potential conflicts to avoid.

---

## Overview

Arche modes are not isolated. Multi-agent organizations (MAOs) and complex systems often benefit from **combining modes strategically**. This guide helps architects decide which modes to integrate and how.

### Key Principles

1. **One primary mode per component** â€” Each agent or subsystem has a dominant mode
2. **Clear boundaries** â€” Mode interaction points should be explicitly defined
3. **Maturity alignment** â€” Mixing mature with experimental modes requires careful integration
4. **Cost awareness** â€” Combining modes increases complexity; benefits must justify it

---

## Mode Compatibility Matrix

### Scoring Legend
- ðŸŸ¢ **Excellent** â€” Natural fit, proven patterns exist
- ðŸŸ¡ **Good** â€” Works well with clear contracts
- ðŸŸ  **Fair** â€” Possible but requires careful design
- ðŸ”´ **Poor** â€” Not recommended without strong justification

### Compatibility Table

|  | 3-Layer | Agentic-Swarm | Event-Driven | RL-Loop | Foundry |
|---|---------|---------------|--------------|---------|---------|
| **3-Layer** | ðŸŸ¢ N/A | ðŸŸ¢ Excellent | ðŸŸ¡ Good | ðŸŸ¡ Good | ðŸŸ¢ Excellent |
| **Agentic-Swarm** | ðŸŸ¢ Excellent | ðŸŸ¢ N/A | ðŸŸ¡ Good | ðŸŸ¡ Good | ðŸŸ¢ Excellent |
| **Event-Driven** | ðŸŸ¡ Good | ðŸŸ¡ Good | ðŸŸ¢ N/A | ðŸŸ¢ Excellent | ðŸŸ¡ Good |
| **RL-Loop** | ðŸŸ¡ Good | ðŸŸ¡ Good | ðŸŸ¢ Excellent | ðŸŸ¢ N/A | ðŸŸ¢ Excellent |
| **Foundry** | ðŸŸ¢ Excellent | ðŸŸ¢ Excellent | ðŸŸ¡ Good | ðŸŸ¢ Excellent | ðŸŸ¢ N/A |

---

## Detailed Pairing Patterns

### ðŸŸ¢ 3-Layer + Agentic-Swarm (Excellent)

**When to combine:**
- Orchestrator needs reliable decision logic (3-Layer)
- Sub-agents need specialization (Agentic-Swarm)
- Coordination rules are deterministic

**Architecture:**
```
Agentic-Swarm (Orchestrator)
â”œâ”€ Sub-agent 1: Deterministic validator (3-Layer)
â”œâ”€ Sub-agent 2: Rule-based router (3-Layer)
â””â”€ Sub-agent 3: Specialized analyzer (3-Layer)
```

**Integration point:** Clear input/output contracts between swarm and each 3-Layer sub-agent

**Example:** Incident Commander (Agentic-Swarm) coordinating Alert Processor, Triage Agent, and Escalation Router (all 3-Layer)

**Risk level:** Low â€” both modes handle clear contracts well

---

### ðŸŸ¢ 3-Layer + Foundry (Excellent)

**When to combine:**
- Individual agents in MAO use 3-Layer implementation
- Foundry orchestrates their learning and evolution

**Architecture:**
```
Foundry (meta-agent scaffold)
â””â”€ 01_agents/
   â”œâ”€ content-reviewer/ (3-Layer directives)
   â”œâ”€ data-processor/ (3-Layer execution)
   â””â”€ api-integrator/ (3-Layer directives)
```

**Integration point:** Foundry's agent manuals specify 3-Layer mode; MODE_INTEGRATION_GUIDE provides guidance

**Example:** All agents in a minimal MAO implemented as 3-Layer directives; Foundry manages learning loop and upgrades

**Risk level:** Low â€” This is Foundry's primary use case

---

### ðŸŸ¢ Event-Driven + RL-Loop (Excellent)

**When to combine:**
- System reacts to events (Event-Driven)
- Strategy for handling events improves over time (RL-Loop)

**Architecture:**
```
Event-Driven (event ingestion & routing)
â””â”€ Event handler: RL-Loop agent
   â”œâ”€ Action space: How to handle event type
   â”œâ”€ Reward signal: Success/failure of handling
   â””â”€ Learning: Improve handling strategy over time
```

**Integration point:** Event-Driven routes events to RL-Loop policy; RL-Loop returns action + updates policy

**Example:** Alert Monitor (Event-Driven) feeds alerts to Priority Optimizer (RL-Loop); learns what priority assignments work best

**Risk level:** Low â€” Natural separation of concerns

---

### ðŸŸ¢ Agentic-Swarm + Foundry (Excellent)

**When to combine:**
- Multi-agent swarm structure (Agentic-Swarm)
- Foundry manages swarm composition and learning

**Architecture:**
```
Foundry (MAO scaffold for swarm)
â””â”€ 01_agents/
   â”œâ”€ orchestrator/ (Agentic-Swarm coordinator)
   â”‚  â”œâ”€ sub-agent: researcher (3-Layer or RL-Loop)
   â”‚  â”œâ”€ sub-agent: analyzer (3-Layer)
   â”‚  â””â”€ sub-agent: synthesizer (3-Layer or Event-Driven)
   â””â”€ 09_learning/ (learning loop for whole swarm)
```

**Integration point:** Foundry's mode-selection-log.md documents swarm structure; patch packs upgrade swarm composition

**Example:** Research Organization (Foundry) containing Chief Researcher (Agentic-Swarm) coordinating 3+ specialized researchers

**Risk level:** Low â€” Foundry is designed for this

---

### ðŸŸ¡ 3-Layer + Event-Driven (Good)

**When to combine:**
- Deterministic handlers for event types (3-Layer)
- Event stream triggers them asynchronously (Event-Driven)

**Architecture:**
```
Event-Driven (event bus)
â””â”€ Event handlers (3-Layer directives)
   â”œâ”€ on_order_received: Process order (directives)
   â”œâ”€ on_payment_failed: Retry logic (directives)
   â””â”€ on_inventory_low: Alert user (directives)
```

**Integration point:** Event handler registry maps event types to 3-Layer directives

**Caution:** 3-Layer expects clear inputs; ensure events are well-structured

**Example:** E-commerce webhook handler: Each event type triggers specific 3-Layer directive

**Risk level:** Medium â€” Requires clean event schema

---

### ðŸŸ¡ 3-Layer + RL-Loop (Good)

**When to combine:**
- Base behavior is deterministic (3-Layer)
- Optimization strategy changes over time (RL-Loop)

**Architecture:**
```
3-Layer base workflow
â””â”€ Decision point: RL-Loop policy
   â”œâ”€ If score high: Execute plan A
   â”œâ”€ If score medium: Execute plan B
   â””â”€ If score low: Execute plan C
   (Policy improves from outcomes)
```

**Integration point:** 3-Layer calls RL-Loop to select among options; logs outcome for learning

**Example:** Content moderator (3-Layer) uses ML model (RL-Loop) trained on past decisions to weight confidence thresholds

**Risk level:** Medium â€” Requires outcome tracking and feedback loops

---

### ðŸŸ¡ Agentic-Swarm + Event-Driven (Good)

**When to combine:**
- Swarm reacts to events (Event-Driven triggers coordination)
- Sub-agents specialize (Agentic-Swarm pattern)

**Architecture:**
```
Event stream
â””â”€ Trigger: Event-Driven
   â””â”€ Coordinate: Agentic-Swarm
      â”œâ”€ Responder 1 (3-Layer)
      â”œâ”€ Responder 2 (Event-Driven)
      â””â”€ Responder 3 (3-Layer)
```

**Integration point:** Event triggers swarm; swarm coordinates async response

**Example:** Alert fires â†’ Incident Commander (Agentic-Swarm) activates response team; each member may use different mode

**Risk level:** Medium â€” Coordination complexity increases with async patterns

---

### ðŸŸ¡ Agentic-Swarm + RL-Loop (Good)

**When to combine:**
- Swarm coordination strategy is learnable (RL-Loop)
- Sub-agents execute specialized tasks

**Architecture:**
```
RL-Loop orchestrator (learns how to coordinate)
â””â”€ Agentic-Swarm (coordination patterns)
   â”œâ”€ Specialist 1
   â”œâ”€ Specialist 2
   â””â”€ Specialist 3
   (Policy: How to assign tasks, sequence them)
```

**Integration point:** RL-Loop policy decides swarm coordination strategy; learns from outcomes

**Example:** Project Manager (RL-Loop) learns optimal task assignment; coordinates team (Agentic-Swarm)

**Risk level:** Medium â€” Two learning systems can interact unexpectedly

---

### ðŸŸ¡ RL-Loop + Foundry (Excellent)

**When to combine:**
- Learning agents inside MAO (RL-Loop)
- Foundry manages learning loop and feedback cycles

**Architecture:**
```
Foundry (MAO with learning agents)
â””â”€ 01_agents/
   â”œâ”€ Recommender (RL-Loop)
   â”œâ”€ Optimizer (RL-Loop)
   â””â”€ Evaluator (3-Layer)
   â””â”€ 09_learning/ (eval scenarios, rubrics for RLagents)
```

**Integration point:** Foundry eval scenarios test RL-Loop policies; feedback-log tracks learning progress

**Example:** Recommendation MAO where multiple agents learn; Foundry manages eval suite and improvement cycles

**Risk level:** Low â€” Foundry is designed for this

---

## Anti-Patterns: Avoid These Combinations

### ðŸ”´ Multiple Uncoordinated RL-Loop Agents

**Problem:** Two RL-Loop agents learning simultaneously can:
- Compete for same reward signals
- Create unstable/oscillating behavior
- Make causality hard to trace

**Solution:**
- Use one RL-Loop with multiple action spaces
- Or use Agentic-Swarm to coordinate them
- Or use Foundry's learning loop to manage interaction

---

### ðŸ”´ Event-Driven + Agentic-Swarm Without Clear Boundaries

**Problem:** Events triggering swarm coordination, but unclear handoff:
- Race conditions in coordination
- Event loss during swarm work
- Deadlock between responders

**Solution:**
- Define explicit state machine for swarm lifecycle
- Use 3-Layer orchestrator wrapper
- Or use Foundry to manage state

---

### ðŸ”´ 3-Layer + Multiple Event-Driven Systems

**Problem:** Deterministic logic responding to multiple async event sources:
- Race conditions on shared state
- Difficult to reason about event ordering
- Hard to test determinism

**Solution:**
- Add event prioritization/queuing layer
- Use 3-Layer to serialize event processing
- Or route events through single Event-Driven system

---

## Migration Patterns: Evolving Modes

### From 3-Layer â†’ Agentic-Swarm

**When:** Single agent becomes too complex; sub-specialization helps

**Steps:**
1. Identify logical sub-tasks in directives
2. Extract each as potential sub-agent
3. Define coordination protocol
4. Test swarm behavior matches original 3-Layer
5. Gradually specialize each sub-agent

**Example:** Content moderator (3-Layer directive) â†’ Chief Moderator (Agentic-Swarm) + 3 specialty validators

---

### From 3-Layer â†’ RL-Loop

**When:** Optimal strategy is not known upfront; learning from outcomes improves results

**Steps:**
1. Identify decision point where learning would help
2. Define action space (options to choose from)
3. Define reward signal (what counts as success)
4. Instrument for outcome tracking
5. Gradually migrate from fixed rules to learned policy

**Example:** Router (3-Layer rules) â†’ Router (RL-Loop policy) learning best routing strategy

---

### From Event-Driven â†’ Event-Driven + RL-Loop

**When:** Handling strategy should improve from handling outcomes

**Steps:**
1. Keep event ingestion/routing as-is (Event-Driven)
2. Wrap handler in RL-Loop agent
3. Define reward from handler success/failure
4. Let policy improve
5. Monitor learning curves

**Example:** Alert handler â†’ Alert handler (RL-Loop) learns best severity assessment

---

### From Agentic-Swarm â†’ Agentic-Swarm + RL-Loop Coordinator

**When:** Swarm coordination strategy should adapt

**Steps:**
1. Keep sub-agents as-is
2. Extract coordinator to RL-Loop agent
3. Define action space (coordination decisions)
4. Define reward (task completion, efficiency)
5. Let coordinator learn best patterns

---

## Integration Checklist

When combining modes, verify:

- [ ] **Clear boundaries** â€” Each mode owns specific responsibilities
- [ ] **Explicit contracts** â€” Input/output specs between modes
- [ ] **Error handling** â€” What happens if one mode fails?
- [ ] **State management** â€” Who owns shared state?
- [ ] **Monitoring** â€” Can you observe each mode separately?
- [ ] **Testing** â€” Can you test each mode in isolation?
- [ ] **Rollback** â€” Can you revert to single mode if needed?
- [ ] **Documentation** â€” Architecture diagram + integration guide
- [ ] **Maturity alignment** â€” âœ… Mature paired with ðŸš€ Emerging documented
- [ ] **Performance** â€” Acceptable latency/throughput with both modes?

---

## Examples: Real Combinations

### Example 1: E-Commerce Platform (All 5 Modes)

```
3-Layer:
â”œâ”€ Order processor (deterministic workflow)
â”œâ”€ Inventory manager (rule-based updates)
â””â”€ Payment handler (reliable transactions)

Event-Driven:
â”œâ”€ Order webhook listener
â”œâ”€ Inventory alert monitor
â””â”€ Payment callback handler

RL-Loop:
â”œâ”€ Product recommender (learns preferences)
â””â”€ Price optimizer (learns demand elasticity)

Agentic-Swarm:
â””â”€ Fulfillment coordinator
   â”œâ”€ Picker (3-Layer)
   â”œâ”€ Packer (3-Layer)
   â””â”€ Shipper (Event-Driven trigger)

Foundry:
â””â”€ (Future) Meta-organization managing all agents
```

**Integration:**
- 3-Layer handles core workflows
- Event-Driven handles webhooks/async
- RL-Loop learns user behavior
- Agentic-Swarm coordinates fulfillment
- All could feed into Foundry learning loop

---

### Example 2: Incident Response (4 Modes)

```
Foundry (Meta-organization):
â””â”€ 01_agents/
   â”œâ”€ Alert Monitor (Event-Driven)
   â”‚  â””â”€ Triggers on: Infrastructure alerts
   â”‚
   â”œâ”€ Incident Commander (Agentic-Swarm)
   â”‚  â”œâ”€ Analyzer (3-Layer diagnosis)
   â”‚  â”œâ”€ Responder (3-Layer executor)
   â”‚  â””â”€ Communicator (3-Layer notifications)
   â”‚
   â”œâ”€ Learning Agent (RL-Loop)
   â”‚  â””â”€ Policy: Learn incident severity classification
   â”‚
   â””â”€ 09_learning/ (Foundry learning loop)
      â”œâ”€ Eval: Did we detect this incident quickly?
      â”œâ”€ Rubric: Incident classification accuracy
      â””â”€ Feedback: Time-to-detection, false positives
```

**Integration:**
- Alert Monitor (Event-Driven) detects incidents
- Routes to Commander (Agentic-Swarm)
- Commander uses Analyzer (3-Layer) to diagnose
- Learning Agent (RL-Loop) improves classifications
- Foundry tracks all improvements, manages upgrades

---

## Next Steps

1. **For your project:** Identify which modes you're combining; check this guide
2. **For Foundry:** Use MODE_INTEGRATION_GUIDE + Compatibility Matrix to recommend mode combos
3. **For community:** Share patterns via GitHub Issues when you discover good combos
4. **For arche:** Over time, add proven patterns as "blessed combinations"

---

## Questions to Ask

- **"Do these modes compete for the same resources?"** â†’ If yes, add coordination layer
- **"Can I test each mode independently?"** â†’ If no, rethink boundaries
- **"What happens if one mode fails?"** â†’ If unclear, add explicit error handling
- **"Does this combination reduce overall complexity?"** â†’ If no, stick with single mode

---

